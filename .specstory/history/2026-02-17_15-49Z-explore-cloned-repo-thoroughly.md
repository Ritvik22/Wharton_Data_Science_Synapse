<!-- Generated by SpecStory -->

<!-- cursor Session task-toolu_019LVtj4KrzaiLGdR3CuLeeF (2026-02-17 15:49Z) -->

# Explore cloned repo thoroughly (2026-02-17 15:49Z)

_**Assistant (2026-02-17 15:49Z • composer-1)**_


Exploring the repository structure and identifying key files.


---

Tool use: **run_terminal_command_v2**

---

Tool use: **run_terminal_command_v2**

---

Tool use: **run_terminal_command_v2**

---


Exploring the repository using file search tools:


---

Tool use: **glob_file_search**

<details>
<summary>Searched codebase "undefined" • **53** results in **1** directory</summary>

Directory: **/tmp/wharton-cloned** (53 files)

| File |
|------|
| `scripts/.gitkeep` |
| `requirements.txt` |
| `outputs/round1_predictions_optimized.csv` |
| `outputs/round1_predictions.csv` |
| `outputs/dataset_summary.csv` |
| `outputs/.gitkeep` |
| `notebooks/catboost_info/time_left.tsv` |
| `notebooks/catboost_info/learn_error.tsv` |
| `notebooks/catboost_info/catboost_training.json` |
| `notebooks/Week1_Data_Exploration.ipynb` |
| `notebooks/04_advancded_features.ipynb` |
| `notebooks/03_ensemble_model.ipynb` |
| `notebooks/02_generate_predictions(LOGREG).ipynb` |
| `notebooks/01_raw_to_game.ipynb` |
| `docs/line_stats.md` |
| `docs/game_aggregation.md` |
| `docs/data_sanity_checks.md` |
| `docs/README.md` |
| `docs/.gitkeep` |
| `data/raw/whl_2025.csv` |
| `data/processed/team_stats.csv` |
| `data/processed/model_input_enhanced.csv` |
| `data/processed/model_input.csv` |
| `data/processed/line_stats.csv` |
| `data/processed/game_stats.csv` |
| `data/processed/.gitkeep` |
| `README.md` |
| `.vscode/settings.json` |
| `.devcontainer/devcontainer.json` |
| `.git/config` |
| `.git/logs/refs/heads/main` |
| `.git/logs/HEAD` |
| `.git/refs/heads/main` |
| `.git/HEAD` |
| `.git/refs/remotes/origin/HEAD` |
| `.git/logs/refs/remotes/origin/HEAD` |
| `.git/packed-refs` |
| `.git/hooks/push-to-checkout.sample` |
| `.git/hooks/update.sample` |
| `.git/hooks/pre-push.sample` |
| `.git/hooks/pre-applypatch.sample` |
| `.git/hooks/pre-merge-commit.sample` |
| `.git/hooks/post-update.sample` |
| `.git/hooks/prepare-commit-msg.sample` |
| `.git/hooks/pre-receive.sample` |
| `.git/hooks/fsmonitor-watchman.sample` |
| `.git/hooks/applypatch-msg.sample` |
| `.git/hooks/pre-commit.sample` |
| `.git/hooks/sendemail-validate.sample` |
| `.git/hooks/pre-rebase.sample` |
| `.git/hooks/commit-msg.sample` |
| `.git/description` |
| `.git/info/exclude` |

</details>

---

Tool use: **glob_file_search**

<details>
<summary>Searched codebase "undefined" • **5** results in **1** directory</summary>

Directory: **/tmp/wharton-cloned** (5 files)

| File |
|------|
| `notebooks/Week1_Data_Exploration.ipynb` |
| `notebooks/04_advancded_features.ipynb` |
| `notebooks/03_ensemble_model.ipynb` |
| `notebooks/02_generate_predictions(LOGREG).ipynb` |
| `notebooks/01_raw_to_game.ipynb` |

</details>

---

Tool use: **glob_file_search**

<details>
<summary>Searched codebase "undefined" • **0** result in **1** directory</summary>

Directory: **/tmp/wharton-cloned** (0 file)

</details>

---

Tool use: **glob_file_search**

<details>
<summary>Searched codebase "undefined" • **9** results in **1** directory</summary>

Directory: **/tmp/wharton-cloned** (9 files)

| File |
|------|
| `outputs/round1_predictions_optimized.csv` |
| `outputs/round1_predictions.csv` |
| `outputs/dataset_summary.csv` |
| `data/raw/whl_2025.csv` |
| `data/processed/team_stats.csv` |
| `data/processed/model_input_enhanced.csv` |
| `data/processed/model_input.csv` |
| `data/processed/line_stats.csv` |
| `data/processed/game_stats.csv` |

</details>

---

Tool use: **glob_file_search**

<details>
<summary>Searched codebase "undefined" • **3** results in **1** directory</summary>

Directory: **/tmp/wharton-cloned** (3 files)

| File |
|------|
| `notebooks/catboost_info/catboost_training.json` |
| `.vscode/settings.json` |
| `.devcontainer/devcontainer.json` |

</details>

---

Tool use: **glob_file_search**

<details>
<summary>Searched codebase "undefined" • **5** results in **1** directory</summary>

Directory: **/tmp/wharton-cloned** (5 files)

| File |
|------|
| `docs/line_stats.md` |
| `docs/game_aggregation.md` |
| `docs/data_sanity_checks.md` |
| `docs/README.md` |
| `README.md` |

</details>

---


Reading notebooks, documentation, and key files:


---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---


Reading documentation files and examining data structure:


---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **ripgrep_raw_search**

---


Examining data files and outputs:


---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---

Tool use: **ripgrep_raw_search**

---


Checking the scripts directory and reviewing the structure:


---

Tool use: **read_file_v2**

---

Tool use: **read_file_v2**

---


## **Comprehensive Repository Analysis**

### **1. Directory Structure**

```
/tmp/wharton-cloned/
├── data/
│   ├── raw/
│   │   └── whl_2025.csv (25,827 rows × 26 columns)
│   └── processed/
│       ├── game_stats.csv (1,312 games)
│       ├── team_stats.csv (32 teams)
│       ├── line_stats.csv (32 teams)
│       ├── model_input.csv (1,312 games × 43 features)
│       └── model_input_enhanced.csv (1,312 games × 65 features)
├── notebooks/
│   ├── 01_raw_to_game.ipynb (Data preprocessing pipeline)
│   ├── 02_generate_predictions(LOGREG).ipynb (Baseline predictions)
│   ├── 03_ensemble_model.ipynb (Model comparison & ensemble)
│   ├── 04_advancded_features.ipynb (Advanced feature engineering)
│   └── Week1_Data_Exploration.ipynb (Initial EDA)
├── outputs/
│   ├── round1_predictions.csv (Baseline predictions)
│   ├── round1_predictions_optimized.csv (Enhanced predictions)
│   ├── baseline_model.pkl
│   ├── baseline_scaler.pkl
│   ├── ensemble_model.pkl
│   └── final_optimized_model.pkl
└── docs/ (Documentation files)
```

### **2. Notebooks and Methodologies**

#### **Notebook 01: `01_raw_to_game.ipynb`** — Data preprocessing pipeline

**Purpose:** Transform raw play-by-play data into game-level and team-level features.

**Methodology:**
1. Load raw data (25,827 records, 26 columns, 1,312 games, 32 teams)
2. Game-level aggregation:
   - Sum goals, xG, shots, assists, penalties per game
   - Create `home_win` binary target
   - Output: `game_stats.csv` (1,312 rows × 15 columns)
3. Team-level aggregation:
   - Aggregate home/away stats separately
   - Compute: games, wins, losses, goals_for, goals_against, goal_diff, win_rate
   - Output: `team_stats.csv` (32 teams × 8 columns)
4. Line efficiency features:
   - First/second offensive line xG and TOI
   - Efficiency: `first_eff = first_xg / first_toi`
   - Disparity: `offensive_disparity = first_eff / second_eff`
   - Output: `line_stats.csv` (32 teams × 8 columns)
5. Model input creation:
   - Merge game stats with home/away team features
   - Create home/away prefixed features
   - Output: `model_input.csv` (1,312 rows × 43 features)
6. Baseline model:
   - Logistic Regression with StandardScaler
   - 28 clean features (excludes leaky columns)
   - AUC: 0.6900

**Features:**
- Win rate, goal differential, xG, shots, penalties
- Line efficiency metrics (first/second offensive lines)
- Offensive disparity (first vs second line efficiency)

---

#### **Notebook 02: `02_generate_predictions(LOGREG).ipynb`** — Baseline predictions

**Purpose:** Generate predictions for Round 1 matchups using the baseline Logistic Regression model.

**Methodology:**
1. Load matchups from Excel (`WHSDSC_Rnd1_matchups.xlsx` - 16 games)
2. Build team feature dictionary:
   - Average team features from all games (home and away)
   - Extract 28 features expected by the trained model
3. Prediction generation:
   - For each matchup, create feature row with home/away team features
   - Scale features using saved scaler
   - Predict home win probability
4. Validation checks:
   - 16 rows, correct columns, probabilities 0-1, no missing values
   - Output: `round1_predictions.csv`

**Results:** 16 predictions with probabilities ranging from 49.4% to 83.7%

---

#### **Notebook 03: `03_ensemble_model.ipynb`** — Model comparison and ensemble

**Purpose:** Compare multiple ML models and create an ensemble.

**Models tested:**
1. Logistic Regression (baseline): AUC 0.6900
2. Random Forest:
   - n_estimators=200, max_depth=8, min_samples_split=20
   - AUC: 0.6747
3. XGBoost:
   - n_estimators=200, max_depth=5, learning_rate=0.05
   - AUC: 0.6586
4. LightGBM:
   - n_estimators=200, max_depth=5, learning_rate=0.05
   - AUC: 0.6273
5. CatBoost:
   - iterations=200, depth=5, learning_rate=0.05
   - AUC: 0.6635

**Ensemble approach:**
- Weighted average using scipy.optimize to maximize AUC
- Optimal weights: Logistic (0.340), RandomForest (0.370), CatBoost (0.283), XGBoost (0.006), LightGBM (0.001)
- Ensemble AUC: 0.6848 (worse than baseline)

**Finding:** Logistic Regression performed best; ensemble did not improve.

---

#### **Notebook 04: `04_advancded_features.ipynb`** — Advanced feature engineering

**Purpose:** Create advanced features and tune models.

**Feature engineering (22 new features):**

1. Interaction features (differences):
   - `win_rate_diff`, `xg_diff`, `shots_diff`, `goal_diff_diff`
   - `first_eff_diff`, `second_eff_diff`, `disparity_diff`

2. Ratio features:
   - `xg_ratio`, `shots_ratio`, `first_eff_ratio`

3. Polynomial features:
   - `home_xg_squared`, `away_xg_squared`, `win_rate_diff_squared`

4. Product features:
   - `home_xg_eff`, `away_xg_eff`
   - `home_strength`, `away_strength`, `strength_diff`

5. Aggregated features:
   - `home_total_line_xg`, `away_total_line_xg`
   - `home_avg_line_eff`, `away_avg_line_eff`

**Total features:** 65 (43 original + 22 new)

**Model tuning:**
- RandomizedSearchCV with 5-fold CV (50 iterations each)
- XGBoost: AUC 0.6537 (CV: 0.6832)
- LightGBM: AUC 0.6489 (CV: 0.6799)
- CatBoost: AUC 0.6838 (CV: 0.6875)
- Stacking Ensemble: AUC 0.6706

**Result:** Baseline Logistic Regression (0.6900) still outperformed tuned models.

**Final model:** CatBoost (best of tuned models) saved as `final_optimized_model.pkl`

**Predictions:** Generated `round1_predictions_optimized.csv` with CatBoost model

---

#### **Notebook 05: `Week1_Data_Exploration.ipynb`** — Initial EDA

**Purpose:** Exploratory data analysis of raw dataset.

**Findings:**
- 25,827 records, 26 columns, 1,312 unique games, 32 teams
- No missing values
- Average 19.7 records per game
- Dataset summary saved to `outputs/dataset_summary.csv`

---

### **3. Scripts**

No standalone Python scripts found. All code is in Jupyter notebooks.

---

### **4. Data Files and Structure**

#### **Raw Data (`whl_2025.csv`):**
- 25,827 rows × 26 columns
- Play-by-play records with:
  - Game identifiers: `game_id`, `record_id`
  - Teams: `home_team`, `away_team`
  - Line information: `home_off_line`, `away_off_line`, `home_def_pairing`, `away_def_pairing`
  - Goalies: `home_goalie`, `away_goalie`
  - Metrics: `toi`, `home_xg`, `away_xg`, `home_shots`, `away_shots`, `home_goals`, `away_goals`
  - Penalties: `home_penalties_committed`, `away_penalties_committed`

#### **Processed Data:**

**`game_stats.csv`:** 1,312 games × 15 columns
- Aggregated game-level statistics

**`team_stats.csv`:** 32 teams × 8 columns
- `team`, `games`, `wins`, `losses`, `goals_for`, `goals_against`, `goal_diff`, `win_rate`
- Top teams: Brazil (70.7%), Netherlands (65.9%), Peru (63.4%)

**`line_stats.csv`:** 32 teams × 8 columns
- First/second line xG, TOI, efficiency
- Offensive disparity metric

**`model_input.csv`:** 1,312 games × 43 features
- Game stats + home/away team features merged

**`model_input_enhanced.csv`:** 1,312 games × 65 features
- Original 43 features + 22 engineered features

---

### **5. Models/Algorithms Used**

#### **Primary model: Logistic Regression**
- AUC: 0.6900
- StandardScaler preprocessing
- 28 features (excludes leaky columns)
- L2 regularization (C=1.0)

#### **Alternative models tested:**
1. Random Forest: AUC 0.6747
2. XGBoost: AUC 0.6586 (tuned: 0.6537)
3. 

---

